{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1> ILI285 - Computación Científica I  / INF285 - Computación Científica </h1>\n",
    "    <h2> Floating Point Arithmetic </h2>\n",
    "    <h2> [[S]cientific [C]omputing [T]eam](#acknowledgements)</h2>\n",
    "    <h2> Version: 1.13</h2>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#intro)\n",
    "* [The nature of floating point numbers](#nature)\n",
    "* [Visualization of floating point numbers](#visualization)\n",
    "* [Loss of significance](#loss)\n",
    "* [Loss of significance in funcion evaluation](#func)\n",
    "* [Another analysis (example from textbook)](#another)\n",
    "* [Acknowledgements](#acknowledgements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='intro' />\n",
    "## Introduction\n",
    "\n",
    "Hello! This notebook is an introduction to how our computers handle the representation of real numbers using double-presicion floating-point format. To understand the contents of this notebook you should have at least a basic notion of how binary numbers work.\n",
    "\n",
    "The aforementioned format occupies 64 bits which are divided as follows:\n",
    "\n",
    "* 1 bit for the sign\n",
    "* 11 bits for the exponent\n",
    "* 52 bits for the mantissa\n",
    "\n",
    "This means that the very next representable number after $1$ is $1 + 2^{-52}$, and their difference, $2^{-52}$, is the $\\epsilon _{mach}$.\n",
    "\n",
    "Additionally, if you'd like to quickly go from a base-2 integer to a base-10 integer and viceversa, Python has some functions that can help you with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int('0b11', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin(2**53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='nature' />\n",
    "## The nature of floating point numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know until now, float representations of real numbers are just a finite and bounded represetation of them. But another interesting thing, is that these floating numbers are distributed across the real numbers. \n",
    "\n",
    "To see that, it's really important to keep in mind the following property:\n",
    "\n",
    "\\begin{equation} \\left|\\frac{\\text{fl}(x)-x}{x}\\right| \\leq \\frac{1}{2} \\epsilon_{\\text{mach}} \\end{equation}\n",
    "\n",
    "where $\\text{fl}(x)$ means the float representation of $x \\in R$. What it says is that **the relative error in representing any non-zero real number x, is bounded by a quantity that depends on the system representation** ($\\epsilon_{\\text{mach}}$).\n",
    "\n",
    "Maybe now you're thinking: what relationship does this have with the distribution of floating point numbers? So if we rewrite the previous property like this:\n",
    "\n",
    "\\begin{equation} |\\text{fl}(x)-x| \\leq \\frac{1}{2} \\epsilon_{\\text{mach}} |x| \\end{equation}\n",
    "\n",
    "it's clearer: **The absolute error (distance) between a real number and its floating point representation is proportional to the real number's magnitude.**\n",
    "\n",
    "Intuitively speaking, if the representation error of a number increases as its magnitude increases, then it's quite natural that **the distance between a floating point number and the next representable floating point number will increase as the magnitude of such number increases (and conversely)**. Could you prove that?  For now we will prove it experimentally.\n",
    "\n",
    "We will use a library named **bitstring** to handle different number representations. You can install it with:\n",
    "```\n",
    "pip install bitstring\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitstring as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two functions are self-explanatory:\n",
    "\n",
    "1. `next_float(f)` computes the next representable float number.\n",
    "2. `epsilon(f)` computes the difference between f and the next representable float number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_float(f):\n",
    "    #packing double-precision foat\n",
    "    b = bs.pack('>d', f)\n",
    "    \n",
    "    #extracting mantisa as unsigned int\n",
    "    #and adding up 1\n",
    "    m = b[12:].uint\n",
    "    m += 1\n",
    "    \n",
    "    #putting the result in his place\n",
    "    b[12:] = m\n",
    "    \n",
    "    return b.float\n",
    "\n",
    "def epsilon(f):\n",
    "    next_f = next_float(f)\n",
    "    return next_f - f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we compute `epsilon(1)` we should get the epsilon machine number. Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prove our hypotesis, we will create an array of values: [1e-32, 1e-31, ..., 1e31, 1e32] and compute their corresponding epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values between 10**-32 and 10**+32\n",
    "values = np.array([10**i for i in range(-32,32)]).astype(float)\n",
    "\n",
    "#corresponding epsilons\n",
    "vepsilon = np.vectorize(epsilon)\n",
    "eps = vepsilon(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(values, eps,'.',markersize=20)\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Corresponding Epsilons')\n",
    "plt.title('Epsilons v/s Values')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(values, eps,'.')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Corresponding Epsilons')\n",
    "plt.title('Epsilons v/s Values')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the hypotesis was right. In other words: Floating point numbers are not linearly distributed across the real numbers, and the distance between them is proportional to their magnitude. **Tiny numbers (~ 0) are closer between each other than bigger numbers are.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='visualization' />\n",
    "## Visualization of floating point numbers\n",
    "\n",
    "With the help of `bitstring` library we could write a function to visualize floating point numbers in his binary representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary(f):\n",
    "    b = bs.pack('>d', f)\n",
    "    b = b.bin\n",
    "    #show sign + exponent + mantisa\n",
    "    print(b[0]+' '+b[1:12]+ ' '+b[12:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some intereseting examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_binary(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int('0b01111111111', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_binary(1.+epsilon(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_binary(+0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_binary(-0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_binary(np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_binary(-np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_binary(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_binary(-np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_binary(2.**-1074)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2.**-1074)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_binary(2.**-1075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2.**-1075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_binary(9.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='loss' />\n",
    "## Loss of significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned, there's a small leap between 1 and the next representable number, which means that if you want to represent a number between those two, you won't be able to do so; that number is nonexistent as it is for the computer, so it'll have to round it to a representable number before storing it in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1.\n",
    "b = 2.**(-52) #emach\n",
    "result_1 = a + b     # arithmetic result is 1.0000000000000002220446049250313080847263336181640625\n",
    "result_1b = result_1-1.0\n",
    "print(\"{0:.1000}\".format(result_1))\n",
    "print(result_1b)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 2.**(-53)\n",
    "result_2 = a + c     # arithmetic result is 1.00000000000000011102230246251565404236316680908203125\n",
    "np.set_printoptions(precision=16)\n",
    "print(\"{0:.1000}\".format(result_2))\n",
    "print(result_2-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_binary(result_2)\n",
    "to_binary(result_2-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2.**(-53) + 2.**(-54)\n",
    "\n",
    "result_3 = a + d     # arithmetic result is 1.000000000000000166533453693773481063544750213623046875\n",
    "print(\"{0:.1000}\".format(result_3))\n",
    "to_binary(result_3)\n",
    "to_binary(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, if you try to save a number between $1$ and $1 + \\epsilon _{mach}$, it will have to be rounded (according to some criteria) to a representable number before being stored, thus creating a difference between the <i>real</i> number and the <i>stored</i> number. This situation is an example of loss of significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does that mean that the \"leap\" between representable numbers is <i>always</i> going to be $\\epsilon _{mach}$? Of course not! Some numbers will require smaller leaps, and some others will require bigger leaps. \n",
    "\n",
    "In any interval of the form $[2^n,2^{n+1}]$ for $n\\in \\mathbb{Z}$ and representables, is constant. For example, all the numbers between $2^{-1}$ and $2^0$ (but excluding $2^0$) have a distance of $\\epsilon _{mach}/2$ between them. All the numbers between $2^0$ and $2^1$ (excluding $2^1$) have a distance of $\\epsilon _{mach}$ between them. Those between $2^1$ and $2^2$ (not including $2^2$) have a distance of $2\\,\\epsilon _{mach}$ between them, and so on and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 2.**(-1)\n",
    "f = b/2. # emach/2\n",
    "\n",
    "result_4 = e + f     # 0.50000000000000011102230246251565404236316680908203125\n",
    "print(\"{0:.1000}\".format(result_4))\n",
    "\n",
    "result_5 = e + b     # 0.5000000000000002220446049250313080847263336181640625\n",
    "print(\"{0:.1000}\".format(result_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = b/4.\n",
    "\n",
    "result_5 = e + g     # 0.500000000000000055511151231257827021181583404541015625\n",
    "print(\"{0:.1000}\".format(result_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll let the students find some representable numbers and some non-representable numbers. It's important to note that loss significance can occur in many more operations and functions other that the simple addition of two numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_1 = a\n",
    "num_2 = b\n",
    "result = a + b\n",
    "print(\"{0:.1000}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='func' />\n",
    "## Loss of significance in function evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss of Significance is present too in the representation of **functions**. A classical example (which you can see in the guide book), is the next function: \n",
    "\n",
    "\\begin{equation}f(x)= \\frac{1 - \\cos x}{\\sin^{2}x} \\end{equation}\n",
    "\n",
    "Applying trigonometric identities, we can obtain the 'equivalent' function:\n",
    "\n",
    "\\begin{equation}f(x)= \\frac{1}{1 + \\cos x} \\end{equation}\n",
    "\n",
    "\n",
    "Both of these functions are apparently equals. Nevertheless, its graphics say to us another thing when $x$ is equal to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(-10,10,0.1)\n",
    "y = (1-np.cos(x))/np.sin(x)**2\n",
    "plt.figure()\n",
    "plt.plot(x,y,'.')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-10,10,0.1)\n",
    "y = 1/(1+np.cos(x))\n",
    "plt.figure()\n",
    "plt.plot(x,y,'.')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-1,1,0.01)\n",
    "y = (1-np.cos(x))/np.sin(x)**2\n",
    "plt.figure()\n",
    "plt.plot(x,y,'.',markersize=20)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 1/(1+np.cos(x))\n",
    "plt.figure()\n",
    "plt.plot(x,y,'.',markersize=20)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens because when $x$ is equal to zero, the first function has an indetermination, but previously, the computer makes a subtraction between numbers that are almost equals. This generates a loss of significance, turning the expression close to this point to be zero. However, modifying this expression to make the second function, eliminates this substraction, fixing the error in its calculation when $x=0$.\n",
    "\n",
    "In conclusion, for us, two representations of a function can be equals, but for the computer they can be different!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='another' />\n",
    "## Another analysis (example from textbook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1 = lambda x: (1.-np.cos(x))/(np.sin(x)**2)\n",
    "f2 = lambda x: 1./(1+np.cos(x))\n",
    "x = np.logspace(-19,0,20)[-1:0:-1]\n",
    "o1 = f1(x)\n",
    "o2 = f2(x)\n",
    "\n",
    "print(\"x,                 f1(x),             f2(x)\")\n",
    "for i in np.arange(len(x)):\n",
    "    print(\"%1.15f, %1.15f, %1.15f\" % (x[i],o1[i],o2[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "### Please make sure you make all of them your BFF!!\n",
    "\n",
    "* Numpy - IEEE 754 Floating Point Special Values: https://docs.scipy.org/doc/numpy-1.10.0/user/misc.html\n",
    "* Matplotlib: http://matplotlib.org/examples/pylab_examples/simple_plot.html\n",
    "* Nice Trick: https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div id='acknowledgements' />\n",
    "# Acknowledgements\n",
    "* _Material originally created by professor Claudio Torres_ (`ctorres@inf.utfsm.cl`) _and assistants: Laura Bermeo, Alvaro Salinas, Axel Simonsen and Martín Villanueva. v.1.1. DI UTFSM. March 2016._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
