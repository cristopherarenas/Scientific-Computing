{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1> ILI285 - Computación Científica I  / INF285 - Computación Científica </h1>\n",
    "    <h2> Generalized Minimal Residual Method </h2>\n",
    "    <h2> [[S]cientific [C]omputing [T]eam](#acknowledgements)</h2>\n",
    "    <h2> Version: 1.2</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#intro)\n",
    "* [Short reminder about Least Squares](#LS)\n",
    "* [GMRes](#GMR)\n",
    "* [Theoretical Problems](#TP)\n",
    "* [Practical Problems](#PP)\n",
    "* [Acknowledgements](#acknowledgements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse.linalg\n",
    "%matplotlib inline\n",
    "#%load_ext memory_profiler\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.size'] = 14\n",
    "mpl.rcParams['axes.labelsize'] = 20\n",
    "mpl.rcParams['xtick.labelsize'] = 14\n",
    "mpl.rcParams['ytick.labelsize'] = 14\n",
    "# from scipy.interpolate import CubicSpline # Starting scipy 0.19.0\n",
    "M=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='intro' />\n",
    "## Introduction\n",
    "\n",
    "Welcome to another edition of our Jupyter Notebooks. A few notebooks back, we saw that the Conjugate Gradient Method, an iterative method, was very useful to solve $A\\,\\mathbf{x}=\\mathbf{b}$ but it only worked when $A$ was positive definite and symmetric. So now we need an iterative method that works with nonsymmetric linear system of equations, and for that we have the Generalized Minimum Residual Method (GMRes). It works really well for finding the solution of large, sparse (and dense as well), nonsymmetric linear systems of equations. Of course, it will also have trouble for ill-conditioned linear system of equations. But it is really easy to add a left or right or both preconditioners!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='LS' />\n",
    "## A quick review on Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Least Squares is used to solve overdetemined linear systems of equations $A\\,\\mathbf{x} = \\mathbf{b}$. That is, for example, a linear system of equations where there are more equations than unknowns. It finds the _best_ $\\overline{\\mathbf{x}}$ so that it minimizes the euclidean length of $\\mathbf{r} = \\mathbf{b} - A\\,\\mathbf{x}$.\n",
    "\n",
    "So, you might be wondering, what does Least Squares have to do with GMRes? WELL, since you're dying to know, I'll tell you: the backward error of the system in GMRes is minimized at each iteration step using a Least Squares formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='GMR' />\n",
    "## GMRes\n",
    "\n",
    "GMRes is a member of the family of Krylov methods. It finds an approximation of $\\mathbf{x}$ restricted to _live_ on the Krylov sub-space $\\mathcal{K_k}$,  where $\\mathcal{K_k}=\\{\\mathbf{r}_0, A\\,\\mathbf{r}_0, A^2\\,\\mathbf{r}_0, \\cdots, A^{k-1}\\,\\mathbf{r}_0\\}$ and $\\mathbf{r}_0 = \\mathbf{b} - A\\,\\mathbf{x}_0$ is the residual vector of the initial guess.\n",
    "\n",
    "The idea behind this method is to look for improvements to the initial guess $\\mathbf{x}_0$ in the Krylov space. At the $k$-th iteration, we enlarge the Krylov space by adding $A^k\\,\\mathbf{r}_0$, reorthogonalize the basis, and then use least squares to find the best improvement to add to $\\mathbf{x}_0$.\n",
    "\n",
    "The algorithm is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Generalized Minimum Residual Method`\n",
    "\n",
    "$\\mathbf{x}_0$ `= initial guess`<br>\n",
    "$\\mathbf{r}$ `=` $\\mathbf{b} - A\\,\\mathbf{x}_0$<br>\n",
    "$\\mathbf{q}_1$ `=` $\\mathbf{r} / \\|\\mathbf{r}\\|_2$<br>\n",
    "`for` $k = 1, ..., m$<br>\n",
    "$\\qquad \\ \\ \\mathbf{y} = A\\,\\mathbf{q}_k$<br>\n",
    "$\\qquad$ `for` $j = 1,2,...,k$ <br>\n",
    "$\\qquad \\qquad$ $h_{jk} = \\mathbf{q}_j^*\\,\\mathbf{y}$<br>\n",
    "$\\qquad \\qquad$ $\\mathbf{y} = \\mathbf{y} - h_{jk}\\, \\mathbf{q}_j$<br>\n",
    "$\\qquad$ `end`<br>\n",
    "$\\qquad \\ h_{k+1,k} = \\|y\\|_2 \\qquad$ `(If ` $h_{k+1,k} = 0$ `, skip next line and terminate at bottom.)` <br>\n",
    "$\\qquad \\ \\mathbf{q}_{k+1} = \\mathbf{y}/h_{k+1,k}$ <br>\n",
    "$\\qquad$ `Minimize` $\\left\\|\\widehat{H}_k\\, \\mathbf{c}_k - [\\|\\mathbf{r}\\|_2 \\ 0 \\ 0 \\ ... \\ 0]^T \\right\\|_2$ `for` $\\mathbf{c}_k$ <br>\n",
    "$\\qquad$ $\\mathbf{x}_k = Q_k \\, \\mathbf{c}_k + \\mathbf{x}_0$ <br>\n",
    "`end`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a very instructive implementation of GMRes.\n",
    "def GMRes(A, b, x0=np.array([0.0]), m=10, flag_display=True, threshold=1e-12):\n",
    "    n = len(b)\n",
    "    if len(x0)==1:\n",
    "        x0=np.zeros(n)\n",
    "    r0 = b - np.dot(A, x0)\n",
    "    nr0=np.linalg.norm(r0)\n",
    "    out_res=np.array(nr0)\n",
    "    Q = np.zeros((n,n))\n",
    "    H = np.zeros((n,n))\n",
    "    Q[:,0] = r0 / nr0\n",
    "    flag_break=False\n",
    "    for k in np.arange(np.min((m,n))):\n",
    "        y = np.dot(A, Q[:,k])\n",
    "        if flag_display:\n",
    "            print('||y||=',np.linalg.norm(y))\n",
    "        for j in np.arange(k+1):\n",
    "            H[j][k] = np.dot(Q[:,j], y)\n",
    "            if flag_display:\n",
    "                print('H[',j,'][',k,']=',H[j][k])\n",
    "            y = y - np.dot(H[j][k],Q[:,j])\n",
    "            if flag_display:\n",
    "                print('||y||=',np.linalg.norm(y))\n",
    "        # All but the last equation are treated equally. Why?\n",
    "        if k+1<n:\n",
    "            H[k+1][k] = np.linalg.norm(y)\n",
    "            if flag_display:\n",
    "                print('H[',k+1,'][',k,']=',H[k+1][k])\n",
    "            if (np.abs(H[k+1][k]) > 1e-16):\n",
    "                Q[:,k+1] = y/H[k+1][k]\n",
    "            else:\n",
    "                print('flag_break has been activated')\n",
    "                flag_break=True\n",
    "            # Do you remember e_1? The canonical vector.\n",
    "            e1 = np.zeros((k+1)+1)        \n",
    "            e1[0]=1\n",
    "            H_tilde=H[0:(k+1)+1,0:k+1]\n",
    "        else:\n",
    "            H_tilde=H[0:k+1,0:k+1]\n",
    "        # Solving the 'SMALL' least square problem. \n",
    "        # This could be improved with Givens rotations!\n",
    "        ck = np.linalg.lstsq(H_tilde, nr0*e1)[0] \n",
    "        if k+1<n:\n",
    "            x = x0 + np.dot(Q[:,0:(k+1)], ck)\n",
    "        else:\n",
    "            x = x0 + np.dot(Q, ck)\n",
    "        # Why is 'norm_small' equal to 'norm_full'?\n",
    "        norm_small=np.linalg.norm(np.dot(H_tilde,ck)-nr0*e1)\n",
    "        out_res = np.append(out_res,norm_small)\n",
    "        if flag_display:\n",
    "            norm_full=np.linalg.norm(b-np.dot(A,x))\n",
    "            print('..........||b-A\\,x_k||=',norm_full)\n",
    "            print('..........||H_k\\,c_k-nr0*e1||',norm_small);\n",
    "        if flag_break:\n",
    "            if flag_display: \n",
    "                print('EXIT: flag_break=True')\n",
    "            break\n",
    "        if norm_small<threshold:\n",
    "            if flag_display:\n",
    "                print('EXIT: norm_small<threshold')\n",
    "            break\n",
    "    return x,out_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,1,0],[0,1,0],[0,1,1]])\n",
    "b = np.array([1,2,3])\n",
    "x0 = np.zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy gmres\n",
    "x_scipy = scipy.sparse.linalg.gmres(A,b,x0)[0]\n",
    "# our gmres\n",
    "x_our, _ = GMRes(A, b)\n",
    "# numpy solve\n",
    "x_np= np.linalg.solve(A,b)\n",
    "\n",
    "# Showing the solutions\n",
    "print('--------------------------------')\n",
    "print('x_scipy',x_scipy)\n",
    "print('x_our',x_our)\n",
    "print('x_np',x_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example, how may iteration does it need to converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0,0,0,1],[1,0,0,0],[0,1,0,0],[0,0,1,0]])\n",
    "b = np.array([1,0,1,0])\n",
    "x_our, _ = GMRes(A, b, m=10)\n",
    "norm_full=np.linalg.norm(b-np.dot(A,x_our))\n",
    "print(norm_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A = np.random.rand(10,10)+10*np.eye(10)\n",
    "b = np.random.rand(10)\n",
    "x_our, out_res = GMRes(A, b, m=10,flag_display=True)\n",
    "norm_full=np.linalg.norm(b-np.dot(A,x_our))\n",
    "print(norm_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the residual over the iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(M,M))\n",
    "plt.semilogy(out_res,'.k',markersize=20,label='residual')\n",
    "plt.grid(True)\n",
    "plt.xlabel(r'$k$')\n",
    "plt.ylabel(r'$\\|\\mathbf{b}-A\\,\\mathbf{x}_k\\|_2$')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='TP' />\n",
    "## Theoretical Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prove that in GMRES method, the backward error $||b- Ax_k||$ decreases monotonically with k.\n",
    "2. What would happen if we pass a singular matrix $A$ to the previous implementation of GMRes?\n",
    "3. Prove that for\n",
    "\\begin{equation}\n",
    "A=\n",
    "\\left[\n",
    "\\begin{array}{c|c}\n",
    "I & C \\\\\n",
    "\\hline\n",
    "0 & I\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}\n",
    "and any $x_0$ and $b$, GMRES converges to the exact solution after **two** steps. Here $C$ is a $m_1 \\times m_2$ submatrix, $0$ denotes the $m_2 \\times m_1$ matrix of zeros, and $I$ denotes the appropiate-sized identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='PP' />\n",
    "## Practical Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. A possible improvement to the present algorithm consists on taking out of the loop the least squares computations, since the _Krylov_ subspace spaned by $Q_k$ doesn't depend on previous least squares calculations.\n",
    "    * Verify the truth of the above statement.\n",
    "    * Verify if it is really an improvement.\n",
    "    * Implement it.\n",
    "    * Test both implementations using `%timeit`\n",
    "1. The GMRES method is meant for huge $n\\times n$ sparse matrices $A$. In most cases, the goal is to run the method for $k$ steps (with $k << n$), reducing the complexity of the subproblems (Least squares). Neverthless for $k$ values too small, the solution $x_k$ could be not as good as needed. So to keep the values $k$ small and avoid bad solutions, there exists a variation of the algorithm known as **Restarted GMRES**: If no enough progress is made toward the solution after $k$ iterations, discard $Q_k$ and start GMRES from the beginning, using the current best guess $x_k$ as the new $x_0$.\n",
    "    * Implement the Restarted GMRES method. Introduce a tolerance parameter to stop restarting.\n",
    "    * Compare the asymptotic operation count and storage requirements of GMRES and Restarted GMRES, for fixed $k$ and increasing $n$.\n",
    "    * Execute it on a _huge_ linear system $A x = b$, and compare the solution with the solution of standard GMRES. Keep a value of $k$ small, and count how many times Restarted GMRES has to restart. Perform benchmarks using `%timeit` and `%memit` and verify the results.\n",
    "    * Describe an example in which Restared GMRES can be expected to fail to converge, whereas GMRES succeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='acknowledgements' />\n",
    "# Acknowledgements\n",
    "* _Material created by professor Claudio Torres_ (`ctorres@inf.utfsm.cl`) _and assistants: Laura Bermeo, Alvaro Salinas, Axel Simonsen and Martín Villanueva. DI UTFSM. April 2016._\n",
    "* _Material updated by professor Claudio Torres_ (`ctorres@inf.utfsm.cl`). DI UTFSM. June 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
